# Description 

Built a Flask web application which performs annotation services for genomics file inputs using AWS services such as EC2, S3, DynamoDB, SQS, SNS, Glacier, SES, & maintained performance with scaling capabilities through AWS Auto Scaling & Cloudwatch.

# gas-framework
An enhanced web framework (based on [Flask](http://flask.pocoo.org/)) for use in the capstone project. Adds robust user authentication (via [Globus Auth](https://docs.globus.org/api/auth)), modular templates, and some simple styling based on [Bootstrap](http://getbootstrap.com/).

Directory contents are as follows:
* `/web` - The GAS web app files
* `/ann` - Annotator files
* `/util` - Utility scripts for notifications, archival, and restoration
* `/aws` - AWS user data files

a)	Description of Archive Process- 
The archive process starts as soon as a job is requested by the free user. A notification is sent to the SNS topic corresponding to the job with its job_id, from the views.py file. The archive queue, in the archive.py file, receives this message, and checks the DynamoDB annotations table to see if the job is completed. If the job is completed, the archive.py ensures 5 minutes has passed since the job complete time before going to the S3 results bucket, downloading the given results file to the local instance, and uploading the file as an archive to glacier. After initiating the upload, the archive.py program deletes the given results file from S3 as well as locally. After this, it updates the given job item in the DynamoDB annotations table, to include the archive_id for later restoration. After this, it goes on to delete the archive request for the given job from the queue. 

The reasoning behind this approach, is the simplicity and intuitiveness involved in using message queues to forward requests as soon as a job is requested, to a program which automatically archives the given job after the specified time interval. Although this approach is simple, this approach is not very reliable since, there is a dependency where all the components must be up and running for the archival process to succeed. Furthermore, there is no mechanism for the archive initiating program to know whether the archival was a success or not, unless there was a notification sent from the archive program back to views.py.  Although SQS itself is a highly scalable service*, owing to its horizontal scaling capabilities (1), this archival process may not be the most scalable solution, as solutions like Amazon SWF are designed for creating scalable workflows. As future improvements on the application, I will experiment with Amazon SWF, as it enables the workflow tasks to be executed in a continuous loop, and notifies the host if a task completed successfully or not. 
*https://aws.amazon.com/blogs/aws/scaling-with-amazon-sqs/

b)	Description of Restoration Process- 
The restoration process starts as soon as a free user registers to become a premium user. A notification is published to the restore SNS topic in views.py. The notification topic sends data to the restore SQS with each of the given userâ€™s files stored in the DynamoDB annotations table. The data includes the archive id, the results file name (under the bucket in which it is stored), as well as the gas-results bucket name, for each of the files corresponding to the given user, which were archived. The restore queue then tries to initiate the restore job from Glacier using the expedited method. If this method fails it gracefully degrades to the standard method. When the restore is first initiated a notification is sent to the thaw notification topic with the filename(s3_key_results_file) and job id. The thaw.py program writes the given filename to a file with the name as its job Id. After the job is completely restored, the initiate_job function sends a notification again to thaw.py, with the archive id, job id and status code. The thaw program now uses the get_job_ouput function to read the data, using the job id as the input. The given data is converted to utf-8 format. Now the program, opens the file written to earlier with the name as job id, and reads in the results file name (s3_key_results_file). It writes the data from the get_job_output function to a file named as the results filename. This file is then uploaded to S3, effectively replacing the file that was deleted upon archival. After this, the program deletes the given archive from glacier. It also removes the two local files stored in the system named as the job_id and the results file. Finally, the program updates the DynamoDb annotations table to remove the archive id for the job. It then deletes the request from the thaw queue. 

The reasoning behind this approach is also the simple and intuitive nature of using message queues to forward requests as different events take place in the application. Again, using this approach of message queues is not the most scalable solution even though SQS itself has very high throughput, and can scale horizontally, since services such Amazon SWF are designed for such purposes and enable highly scalable workflows. 
